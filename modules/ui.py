# modules/ui.py
import streamlit as st

def display_ui():
    """
    Streamlit UI ì»´í¬ë„ŒíŠ¸ë¥¼ í‘œì‹œí•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    st.set_page_config(page_title="RAG ê¸°ë°˜ LLM ë¹„êµ í‰ê°€ ë„êµ¬", layout="wide")
    st.title("RAG ê¸°ë°˜ LLM ë¹„êµ í‰ê°€ ë„êµ¬")
    st.markdown("""
    PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, ì„ íƒí•œ LLM ëª¨ë¸ë“¤ì˜ ë‹µë³€ì„ ë¹„êµí•˜ì—¬ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)

    with st.sidebar:
        st.header("1. íŒŒì¼ ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader("ë¹„êµí•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

        st.header("2. ëª¨ë¸ ì„ íƒ")
        selected_models = {
            "Gemini 2.5 Pro": st.checkbox("Gemini 2.5 Pro", value=True),
            "GPT-4o": st.checkbox("GPT-4o", value=True),
            "Claude 4 Opus": st.checkbox("Claude 4Opus", value=True),
        }

    question = st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

    return uploaded_file, question, selected_models

def display_results(responses):
    """
    LLM ì‘ë‹µ ê²°ê³¼ë¥¼ ì»¬ëŸ¼ í˜•íƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    # ì‘ë‹µì´ ìˆëŠ” ëª¨ë¸ì˜ ìˆ˜ë§Œí¼ ì»¬ëŸ¼ ìƒì„±
    valid_responses = {k: v for k, v in responses.items() if v}
    if not valid_responses:
        st.warning("í‘œì‹œí•  ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    cols = st.columns(len(valid_responses))
    for i, (model_name, result) in enumerate(valid_responses.items()):
        with cols[i]:
            st.subheader(f"ğŸ¤– {model_name}")
            st.info(f"ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ")
            st.markdown(result["answer"])
            with st.expander("ì°¸ê³ í•œ ì»¨í…ìŠ¤íŠ¸ ë³´ê¸°"):
                for doc in result["source_documents"]:
                    st.markdown(f"**í˜ì´ì§€ {doc.metadata.get('page', 'N/A')}:**")
                    st.caption(doc.page_content)